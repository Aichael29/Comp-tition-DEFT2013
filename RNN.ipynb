{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ad552d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 223ms/step - accuracy: 0.6757 - loss: 0.7004 - val_accuracy: 0.8437 - val_loss: 0.3654\n",
      "Epoch 2/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 188ms/step - accuracy: 0.8827 - loss: 0.2818 - val_accuracy: 0.8569 - val_loss: 0.3368\n",
      "Epoch 3/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 182ms/step - accuracy: 0.9045 - loss: 0.2211 - val_accuracy: 0.8569 - val_loss: 0.3279\n",
      "Epoch 4/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 120ms/step - accuracy: 0.9313 - loss: 0.1885 - val_accuracy: 0.8521 - val_loss: 0.3671\n",
      "Epoch 5/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 143ms/step - accuracy: 0.9306 - loss: 0.1804 - val_accuracy: 0.8525 - val_loss: 0.4042\n",
      "Epoch 6/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 165ms/step - accuracy: 0.9482 - loss: 0.1407 - val_accuracy: 0.8421 - val_loss: 0.4374\n",
      "Epoch 7/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 168ms/step - accuracy: 0.9591 - loss: 0.1164 - val_accuracy: 0.8449 - val_loss: 0.4732\n",
      "Epoch 8/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 172ms/step - accuracy: 0.9694 - loss: 0.0923 - val_accuracy: 0.8381 - val_loss: 0.5474\n",
      "Epoch 9/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 161ms/step - accuracy: 0.9730 - loss: 0.0821 - val_accuracy: 0.8377 - val_loss: 0.5875\n",
      "Epoch 10/10\n",
      "\u001b[1m312/312\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 162ms/step - accuracy: 0.9781 - loss: 0.0675 - val_accuracy: 0.8261 - val_loss: 0.5905\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 73ms/step\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "       Dessert      0.961     0.953     0.957       726\n",
      "        Entrée      0.675     0.642     0.658       611\n",
      "Plat principal      0.818     0.844     0.831      1158\n",
      "\n",
      "      accuracy                          0.826      2495\n",
      "     macro avg      0.818     0.813     0.815      2495\n",
      "  weighted avg      0.825     0.826     0.825      2495\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Charger les données\n",
    "train_path = 'C:/Users/dell latitude 7400/Downloads/train.csv'\n",
    "train_data = pd.read_csv(train_path)\n",
    "\n",
    "# Prétraitement des données\n",
    "tokenizer = Tokenizer(num_words=10000, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_data['titre'] + \" \" + train_data['recette'])\n",
    "sequences = tokenizer.texts_to_sequences(train_data['titre'] + \" \" + train_data['recette'])\n",
    "\n",
    "# Padding des séquences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encodage des labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(train_data['type'])\n",
    "labels_categorical = to_categorical(labels)\n",
    "\n",
    "# Séparation des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels_categorical, test_size=0.2, random_state=42)\n",
    "\n",
    "# Construction du modèle\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=10000, output_dim=64),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(labels_categorical.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Entraînement du modèle\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))\n",
    "\n",
    "# Prédiction sur l'ensemble de test\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Évaluation du modèle\n",
    "print(classification_report(y_true_classes, y_pred_classes, target_names=label_encoder.classes_, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7eaf48e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
